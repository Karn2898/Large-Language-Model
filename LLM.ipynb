{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I am going to build a Large Language model and yeah ..that's it"
      ],
      "metadata": {
        "id": "8Q9HSdrLJ-zH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.EDA\n"
      ],
      "metadata": {
        "id": "BIyi9AozKHPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akS-sRrQJuv3",
        "outputId": "644473b6-a11b-4067-ca21-1368d0bf7613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.9.0+cpu\n",
            "tiktoken version: 0.12.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "JXyov0osKqm2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIb2Qi8yKvlD",
        "outputId": "96e3b335-3220-40b5-eb7c-75b9fee201a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextdatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "HsaSPUmAqIw7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        " def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "2mNyR9BTq5nY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
        "                         shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    dataset = TextdatasetV1atasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "cTERKssWq8zS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "context_length = 1024\n",
        "\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
        "                         shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Corrected typo here\n",
        "    dataset = TextdatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "batch_size = 8\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=batch_size,\n",
        "    max_length=max_length,\n",
        "    stride=max_length\n",
        ")"
      ],
      "metadata": {
        "id": "YAi3TI81rdKy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " for batch in dataloader:\n",
        "  x,y = batch\n",
        "\n",
        "  token_embeddingd=token_embedding_layer(x)\n",
        "  pos_embeddings=pos_embedding_layer(torch.arange(max_length))\n",
        "\n",
        "  input_embeddings=token_embeddingd+pos_embeddings\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "id": "C499CIy0rf_v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktu_oXIhwiFM",
        "outputId": "1cb80610-1d71-4b43-9607-0bc078ca89a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Attention Mechanisms"
      ],
      "metadata": {
        "id": "IwrJj5wIxtvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "multi head attention"
      ],
      "metadata": {
        "id": "A7pTljgU4zEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn # Import torch.nn as nn\n",
        "\n",
        "class CausalSelfAttention(nn.Module): # Corrected to nn.Module\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "    super().__init__() # Missing super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) # Corrected to W_key\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    # Mask definition for causal attention\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1).bool())\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, n_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(1, 2) # (b, n_tokens, n_tokens)\n",
        "    attn_scores.masked_fill_(self.mask[:n_tokens, :n_tokens], -torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores / (keys.shape[-1]**0.5), dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec # Corrected variable name\n",
        "\n",
        "class MultiHeadAttention(nn.Module): # Corrected class name and nn.Module\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads # Store num_heads\n",
        "    self.d_out = d_out # Store d_out for output projection\n",
        "    # Each head processes d_in, but output d_out/num_heads. Assuming d_out is total output dim.\n",
        "    # Let's adjust CausalSelfAttention to take d_in for its outputs to be consistent.\n",
        "    # If d_out is the dimension per head, then d_in should be passed to CausalSelfAttention\n",
        "    # If d_out is the total output dimension, then each head should output d_out // num_heads\n",
        "    # For simplicity, let's assume d_in is passed to CausalSelfAttention and its output will be concatenated\n",
        "    # Then, d_out_per_head should be d_out\n",
        "    self.heads = nn.ModuleList(\n",
        "        [CausalSelfAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "         for _ in range(num_heads)] # Corrected for loop and variable name\n",
        "    )\n",
        "    self.out_proj = nn.Linear(num_heads * d_out, d_out) # Added output projection layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Each head outputs d_out, so concatenation results in num_heads * d_out\n",
        "    context_vecs = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "    # Project the concatenated output back to d_out\n",
        "    return self.out_proj(context_vecs)\n"
      ],
      "metadata": {
        "id": "-C6JBj3H5WMl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "d_in = output_dim\n",
        "d_out = output_dim\n",
        "num_heads = 8\n",
        "context_length = max_length\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads)\n",
        "\n",
        "batch = input_embeddings\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GVuZwHy4vkh",
        "outputId": "c97db1dd-75d3-47bd-c8f6-1ef21614d783"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZtVGTWnf8iyB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}