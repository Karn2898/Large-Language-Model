{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I am going to build a Large Language model and yeah ..that's it"
      ],
      "metadata": {
        "id": "8Q9HSdrLJ-zH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.EDA\n"
      ],
      "metadata": {
        "id": "BIyi9AozKHPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akS-sRrQJuv3",
        "outputId": "a4848b4f-ec84-42f9-e785-af39a0ed571f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.9.0+cpu\n",
            "tiktoken version: 0.12.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "JXyov0osKqm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIb2Qi8yKvlD",
        "outputId": "8a07dcc8-417d-45ab-c2ff-92206c818f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextdatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "HsaSPUmAqIw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        " def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "2mNyR9BTq5nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
        "                         shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    dataset = TextdatasetV1atasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "cTERKssWq8zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "context_length = 1024\n",
        "\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
        "                         shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Corrected typo here\n",
        "    dataset = TextdatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "batch_size = 8\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=batch_size,\n",
        "    max_length=max_length,\n",
        "    stride=max_length\n",
        ")"
      ],
      "metadata": {
        "id": "YAi3TI81rdKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " for batch in dataloader:\n",
        "  x,y = batch\n",
        "\n",
        "  token_embeddingd=token_embedding_layer(x)\n",
        "  pos_embeddings=pos_embedding_layer(torch.arange(max_length))\n",
        "\n",
        "  input_embeddings=token_embeddingd+pos_embeddings\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "id": "C499CIy0rf_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktu_oXIhwiFM",
        "outputId": "4b327101-568e-4ad9-e45e-8d29d5936586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Attention Mechanisms"
      ],
      "metadata": {
        "id": "IwrJj5wIxtvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "multi head attention"
      ],
      "metadata": {
        "id": "A7pTljgU4zEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1).bool())\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, n_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(1, 2)\n",
        "    attn_scores.masked_fill_(self.mask[:n_tokens, :n_tokens], -torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores / (keys.shape[-1]**0.5), dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_out = d_out\n",
        "    self.heads = nn.ModuleList(\n",
        "        [CausalSelfAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "         for _ in range(num_heads)]\n",
        "    )\n",
        "    self.out_proj = nn.Linear(num_heads * d_out, d_out)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    context_vecs = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "    return self.out_proj(context_vecs)"
      ],
      "metadata": {
        "id": "-C6JBj3H5WMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "d_in = output_dim\n",
        "d_out = output_dim\n",
        "num_heads = 8\n",
        "context_length = max_length\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads)\n",
        "\n",
        "batch = input_embeddings\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GVuZwHy4vkh",
        "outputId": "c97db1dd-75d3-47bd-c8f6-1ef21614d783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the GPT text to generate words"
      ],
      "metadata": {
        "id": "ku9TewqRGg-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps=1e-5\n",
        "    self.scale=nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean=x.mean(dim=-1,keepdim=True)\n",
        "    var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
        "    norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
        "    return self.scale*norm_x+self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 0.5 * x * (1+torch.tanh(\n",
        "        torch.sqrt(torch.tensor(2.0/torch.pi))*\n",
        "        (x+0.044715 * torch.pow(x,3))\n",
        "    ))"
      ],
      "metadata": {
        "id": "ZtVGTWnf8iyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers=nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.att=MultiHeadAttention(\n",
        "        d_in=cfg[\"emb_dim\"],\n",
        "        d_out=cfg[\"emb_dim\"],\n",
        "        context_length=cfg[\"context_length\"],\n",
        "        num_heads=cfg[\"n_heads\"],\n",
        "        dropout=cfg[\"drop_rate\"],\n",
        "        qkv_bias=cfg[\"qkv_bias\"]\n",
        "    )\n",
        "\n",
        "    self.ff=FeedForward(cfg)\n",
        "    self.norm1=LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2=LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut=nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "nfioRdz4Icvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "    )\n",
        "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "  def generate_text_Simple(self, idx, max_new_tokens, context_length):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -context_length:]\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits = self(idx_cond)\n",
        "\n",
        "\n",
        "      logits = logits[:, -1, :]\n",
        "      idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "      idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "uRIK4TT2K60x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "def main():\n",
        "  GPT_CONFIG_124M={\n",
        "    \"vocab_size\": 50257,\n",
        "        \"context_length\": 1024,\n",
        "        \"emb_dim\": 768,\n",
        "        \"n_heads\": 12,\n",
        "        \"n_layers\": 12,\n",
        "        \"drop_rate\": 0.1,\n",
        "        \"qkv_bias\": False\n",
        "  }\n",
        "\n",
        "  torch.manual_seed(123)\n",
        "  model=GPTModel(GPT_CONFIG_124M)\n",
        "  model.eval()\n",
        "\n",
        "  start_context=\"Hello, I am\"\n",
        "\n",
        "  tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
        "  encoded=tokenizer.encode(start_context)\n",
        "  encoded_tensor=torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "  print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
        "  print(\"\\nInput text:\", start_context)\n",
        "  print(\"Encoded input text:\", encoded)\n",
        "  print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
        "\n",
        "  out=model.generate_text_Simple(\n",
        "      idx=encoded_tensor,\n",
        "      max_new_tokens=10,\n",
        "      context_length=GPT_CONFIG_124M[\"context_length\"]\n",
        "  )\n",
        "\n",
        "  decoded_text=tokenizer.decode(out.squeeze(0).tolist())\n",
        "  print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
        "  print(\"\\nOutput:\", out)\n",
        "  print(\"Output length:\", len(out[0]))\n",
        "  print(\"Output text:\", decoded_text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfvPRvB2NwPJ",
        "outputId": "08e96d9a-330e-462b-d865-cf531fccb19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "                      IN\n",
            "==================================================\n",
            "\n",
            "Input text: Hello, I am\n",
            "Encoded input text: [15496, 11, 314, 716]\n",
            "encoded_tensor.shape: torch.Size([1, 4])\n",
            "\n",
            "\n",
            "==================================================\n",
            "                      OUT\n",
            "==================================================\n",
            "\n",
            "Output: tensor([[15496,    11,   314,   716,  1686, 17362, 37021,  6805, 33311, 40953,\n",
            "         29515, 46248,  6723,  5409]])\n",
            "Output length: 14\n",
            "Output text: Hello, I amaps BaseballEdge components Keeping loosenetsksomeone SS decide\n"
          ]
        }
      ]
    }
  ]
}